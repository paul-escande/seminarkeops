{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seminar_Keops.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VSrr9SDR-cx"
      },
      "source": [
        "# PyKeops\n",
        "\n",
        "This notebook gathers some examples available on the Keops website (https://www.kernel-operations.io/keops/_auto_examples/index.html#)\n",
        "\n",
        "The notebook is organized as:\n",
        "- I/ installation procedure\n",
        "- II/ First example to play with the LazyTensors\n",
        "- III/ Second example to play with the LazyTensors\n",
        "- Then three applications are presented (k-NN on MNIST, kernel interpolation, k-means). Feel free to play with them depending on your interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgbooE5SSHpI"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Don't forget to swith GPU accelerator on: **Runtime>Change runtime type>Hardware accelerator** to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CgOM3f3KqCo"
      },
      "source": [
        "!pip install pykeops[colab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlCZleFdSJcO"
      },
      "source": [
        "## First example with LazyTensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqcFzBlTs3u"
      },
      "source": [
        "### Straight to Keops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcIXgArsNfyZ"
      },
      "source": [
        "import torch\n",
        "\n",
        "N,M,D = 10**5, 10**5, 10\n",
        "x = torch.rand(N,D).cuda()\n",
        "y = torch.rand(M,D).cuda()\n",
        "\n",
        "# turning Tensors into Keops symbolic variables\n",
        "from pykeops.torch import LazyTensor\n",
        "import time\n",
        "x_i = LazyTensor(x[:,None,:]) # x_i.shape = (N,1,D)\n",
        "y_j = LazyTensor(y[None,:,:]) # y_j.shape = (1,M,D)\n",
        "D_ij = ((x_i-y_j)**2).sum(dim=2) # symbolic (N,M,1) matrix of squarred distances\n",
        "# Note that nothing has been computed yet, everything will be done in the final reduction step\n",
        "\n",
        "# first time called triggers the compilation + Warming up GPU\n",
        "indices_i = D_ij.argmin(dim=1)\n",
        "\n",
        "start = time.perf_counter()\n",
        "indices_i = D_ij.argmin(dim=1) \n",
        "torch.cuda.synchronize()\n",
        "end = time.perf_counter()\n",
        "\n",
        "print(end-start)\n",
        "print(s_i[:5])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQGNbGcFTrWu"
      },
      "source": [
        "### Comparison with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OswQpnAwTqZY"
      },
      "source": [
        "x_i = x[:, None, :]  # (M, 1, D) torch array\n",
        "y_j = y[None, :, :]  # (1, N, D) torch array\n",
        "\n",
        "start = time.perf_counter()\n",
        "D_ij = ((x_i - y_j) ** 2).sum(-1)  # (M, N) array of squared distances |x_i-y_j|^2\n",
        "s_i = D_ij.argmin(dim=1)  # (M,)   array of integer indices\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "end = time.perf_counter()\n",
        "print(end-start)\n",
        "print(s_i[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biYxiO93VJTo"
      },
      "source": [
        "Fell free to play with M,N,D to see memory overflow and timing differences !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DInfgeviTiAh"
      },
      "source": [
        "## Second example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYMI_ChlTlfN"
      },
      "source": [
        "M,N = (100000,200000)\n",
        "D = 3\n",
        "x = torch.randn(M, D).cuda()  # M target points in dimension D, stored on the GPU\n",
        "y = torch.randn(N, D).cuda()  # N source points in dimension D, stored on the GPU\n",
        "b = torch.randn(N, 4).cuda()  # N values of the 4D source signal, stored on the GPU\n",
        "\n",
        "# x.requires_grad = True  # In the next section, we'll compute gradients wrt. x!\n",
        "\n",
        "from pykeops.torch import Vi, Vj # nice utilities\n",
        "\n",
        "x_i = Vi(x) # (M, 1, D) LazyTensor, equivalent to LazyTensor( x[:,None,:] )\n",
        "y_j = Vj(y) # (1, N, D) LazyTensor, equivalent to LazyTensor(y[None, :, :])  \n",
        "b_j = Vj(b) # (1, N, D) LazyTensor, equivalent to LazyTensor(b[None, :, :])  \n",
        "sigma = 0.5\n",
        "\n",
        "D_ij = ((x_i - y_j) ** 2 / sigma**2).sum(-1) # symbolic (M,N) matrix\n",
        "K_ij = (-D_ij).exp()  # Symbolic (M, N) Gaussian kernel\n",
        "\n",
        "a_i = K_ij @ b  # arming up and compilation\n",
        "\n",
        "start = time.perf_counter()\n",
        "a_i = K_ij @ b  # The matrix-vector product \"@\" can be used on \"raw\" PyTorch tensors!\n",
        "torch.cuda.synchronize()\n",
        "end = time.perf_counter()\n",
        "\n",
        "print(end-start)\n",
        "print(\"a_i is now a {} of shape {}.\".format(type(a_i), a_i.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRwL82QlYCh8"
      },
      "source": [
        "## k-nn on MNIST\n",
        "\n",
        "Classification of digits using nearest neighbor search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFV19M1yYZLp"
      },
      "source": [
        "Loading MNIST database: 70,000 images (28,28) with labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL7z2AiMYFr9"
      },
      "source": [
        "from pykeops.torch import LazyTensor, Vi, Vj\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml(\"mnist_784\", cache=False)\n",
        "\n",
        "x = torch.tensor(mnist.data.astype(\"float32\"))\n",
        "y = torch.tensor(mnist.target.astype(\"int64\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMgiVtY8YjYO"
      },
      "source": [
        "Split train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gabK8OTYYlcT"
      },
      "source": [
        "D = x.shape[1]\n",
        "Ntrain, Ntest = (60000, 10000)\n",
        "# using continuous is crucial for keops. It raise an error if the data is not\n",
        "x_train, y_train = x[:Ntrain, :].contiguous(), y[:Ntrain].contiguous()\n",
        "x_test, y_test = (\n",
        "    x[Ntrain : Ntrain + Ntest, :].contiguous(),\n",
        "    y[Ntrain : Ntrain + Ntest].contiguous(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR2akj_sYykE"
      },
      "source": [
        "Perform the K-NN classification on 10,000 test images in dimension 784:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJRTXjoEY3bx"
      },
      "source": [
        "K = 3  # N.B.: K has very little impact on the running time\n",
        "\n",
        "\n",
        "X_i = Vi(x_test) # (10000, 1, 784) test set\n",
        "X_j = Vj(x_train)  # (1, 60000, 784) train set\n",
        "D_ij = ((X_i - X_j) ** 2).sum(-1)  # (10000, 60000) symbolic matrix \n",
        "\n",
        "# compilation\n",
        "ind_knn = D_ij.argKmin(K, dim=1) \n",
        "\n",
        "start = time.perf_counter()\n",
        "ind_knn = D_ij.argKmin(K, dim=1)  # Samples <-> Dataset, (N_test, K)\n",
        "lab_knn = y_train[ind_knn]  # (N_test, K) array of integers in [0,9]\n",
        "y_knn, _ = lab_knn.mode()  # Compute the most likely label\n",
        "end = time.perf_counter()\n",
        "\n",
        "error = (y_knn != y_test).float().mean().item()\n",
        "\n",
        "print(\n",
        "    \"{}-NN on the full MNIST dataset: test error = {:.2f}% in {:.2f}s.\".format(\n",
        "        K, error * 100, end-start\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-e-PKazZ7ws"
      },
      "source": [
        "Display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bskg4LKMZ6dS"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "for i in range(6):\n",
        "    ax = plt.subplot(2, 3, i + 1)\n",
        "    ax.imshow((255 - x_test[i]).view(28, 28).detach().cpu().numpy(), cmap=\"gray\")\n",
        "    ax.set_title(\"label = {}\".format(y_knn[i].int()))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A3bAkec7Zpk"
      },
      "source": [
        "## Kernel interpolation\n",
        "\n",
        "Sovling $a^* = \\underset{a}{\\textrm{argmin}} \\| (\\alpha Id + K)a -b \\|_2^2$\n",
        "where $K$ is a symetric definite linear opeartor and $\\alpha > 0$\n",
        "\n",
        "Generate some data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iNMA2sr7icT"
      },
      "source": [
        "import torch\n",
        "from pykeops.torch import LazyTensor, Vi, Vj\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "\n",
        "N = 10000\n",
        "# Sampling locations:\n",
        "x = torch.rand(N, 2).type(dtype)\n",
        "\n",
        "# Some random-ish 2D signal:\n",
        "b = ((x - 0.5) ** 2).sum(1, keepdim=True)\n",
        "b[b > 0.4 ** 2] = 0\n",
        "b[b < 0.3 ** 2] = 0\n",
        "b[b >= 0.3 ** 2] = 1\n",
        "b = b + 0.05 * torch.randn(N, 1).type(dtype)\n",
        "\n",
        "# Add 25% of outliers:\n",
        "Nout = N // 4\n",
        "b[-Nout:] = torch.rand(Nout, 1).type(dtype)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abotNL0i9Er5"
      },
      "source": [
        "Define our regression model: Laplacian kernel of deviation sigma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUnPckls9Clr"
      },
      "source": [
        "def laplacian_kernel(x, y, sigma=0.1):\n",
        "    x_i = LazyTensor(x[:, None, :])  # (M, 1, 1)\n",
        "    y_j = LazyTensor(y[None, :, :])  # (1, N, 1)\n",
        "    D_ij = ((x_i - y_j) ** 2).sum(-1)  # (M, N) symbolic matrix of squared distances\n",
        "    return (-D_ij.sqrt() / sigma).exp()  # (M, N) symbolic Laplacian kernel matrix\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oCsad0Z9Me1"
      },
      "source": [
        "Perform the kernel interpolation using the solve routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo3kTgbS9LXE"
      },
      "source": [
        "alpha = 10  # Ridge regularization\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "K = laplacian_kernel(x, x)\n",
        "a = K.solve(b, alpha=alpha) # remember first time call for compilation\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\n",
        "    \"Time to perform an RBF interpolation with {:,} samples in 2D: {:.5f}s\".format(\n",
        "        N, end - start\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sn-2FGA-EdK"
      },
      "source": [
        "# Extrapolate on a uniform sample:\n",
        "X = Y = torch.linspace(0, 1, 101).type(dtype)\n",
        "X, Y = torch.meshgrid(X, Y)\n",
        "t = torch.stack((X.contiguous().view(-1), Y.contiguous().view(-1)), dim=1)\n",
        "\n",
        "K_tx = laplacian_kernel(t, x)\n",
        "mean_t = K_tx @ a\n",
        "mean_t = mean_t.view(101, 101)\n",
        "\n",
        "# 2D plot: noisy samples and interpolation in the background\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.scatter(\n",
        "    x.cpu()[:, 0], x.cpu()[:, 1], c=b.cpu().view(-1), s=25000 / len(x), cmap=\"bwr\"\n",
        ")\n",
        "plt.imshow(\n",
        "    mean_t.cpu().numpy()[::-1, :],\n",
        "    interpolation=\"bilinear\",\n",
        "    extent=[0, 1, 0, 1],\n",
        "    cmap=\"coolwarm\",\n",
        ")\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-HHRh1Q_PWg"
      },
      "source": [
        "## K-means algorithm\n",
        "\n",
        "Goal: use the bruteforce NN search to implement a large-scale K-means clustering wo/ memory overflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4P6OSyo_eDc"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from pykeops.torch import LazyTensor\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "dtype = torch.float32 if use_cuda else torch.float64"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyyHuX-a_jyP"
      },
      "source": [
        "def KMeans(x, K=10, Niter=10, verbose=True):\n",
        "    \"\"\"Implements Lloyd's algorithm for the Euclidean metric.\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    N, D = x.shape  # Number of samples, dimension of the ambient space\n",
        "\n",
        "    c = x[:K, :].clone()  # Simplistic initialization for the centroids\n",
        "\n",
        "    x_i = LazyTensor(x.view(N, 1, D))  # (N, 1, D) samples\n",
        "    c_j = LazyTensor(c.view(1, K, D))  # (1, K, D) centroids\n",
        "\n",
        "    # K-means loop:\n",
        "    # - x  is the (N, D) point cloud,\n",
        "    # - cl is the (N,) vector of class labels\n",
        "    # - c  is the (K, D) cloud of cluster centroids\n",
        "    for i in range(Niter):\n",
        "\n",
        "        # E step: assign points to the closest cluster -------------------------\n",
        "        D_ij = ((x_i - c_j) ** 2).sum(-1)  # (N, K) symbolic squared distances\n",
        "        cl = D_ij.argmin(dim=1).long().view(-1)  # Points -> Nearest cluster\n",
        "\n",
        "        # M step: update the centroids to the normalized cluster average: ------\n",
        "        # Compute the sum of points per cluster:\n",
        "        c.zero_()\n",
        "        c.scatter_add_(0, cl[:, None].repeat(1, D), x)\n",
        "\n",
        "        # Divide by the number of points per cluster:\n",
        "        Ncl = torch.bincount(cl, minlength=K).type_as(c).view(K, 1)\n",
        "        c /= Ncl  # in-place division to compute the average\n",
        "\n",
        "    if verbose:  # Fancy display -----------------------------------------------\n",
        "        if use_cuda:\n",
        "            torch.cuda.synchronize()\n",
        "        end = time.time()\n",
        "        print(\n",
        "            f\"K-means for the Euclidean metric with {N:,} points in dimension {D:,}, K = {K:,}:\"\n",
        "        )\n",
        "        print(\n",
        "            \"Timing for {} iterations: {:.5f}s = {} x {:.5f}s\\n\".format(\n",
        "                Niter, end - start, Niter, (end - start) / Niter\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return cl, c"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjALqBxd_09I"
      },
      "source": [
        "First xp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ys2iveU_2IO"
      },
      "source": [
        "N, D, K = 10000, 2, 50\n",
        "x = 0.7 * torch.randn(N, D, dtype=dtype) + 0.3\n",
        "cl, c = KMeans(x, K)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(x[:, 0].cpu(), x[:, 1].cpu(), c=cl.cpu(), s=30000 / len(x), cmap=\"tab10\")\n",
        "plt.scatter(c[:, 0].cpu(), c[:, 1].cpu(), c=\"black\", s=50, alpha=0.8)\n",
        "plt.axis([-2, 2, -2, 2])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGtin-aEALW4"
      },
      "source": [
        "Second cp: N=10^6 in D=100 w/ K=1,000 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcLyCkMLAU7B"
      },
      "source": [
        "N, D, K = 1000000, 100, 1000\n",
        "x = torch.randn(N, D, dtype=dtype)\n",
        "cl, c = KMeans(x, K)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}